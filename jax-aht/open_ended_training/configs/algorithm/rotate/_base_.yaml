# @package algorithm
# ^ tells hydra to place these value directly under algorithm key
ALG: rotate
TIMESTEPS_PER_ITER_PARTNER: 1.5e6 # per iter of open-ended training, divided between conf-br and conf-ego and between all partner seeds
TIMESTEPS_PER_ITER_EGO: 1e6
NUM_OPEN_ENDED_ITERS: 30
NUM_CHECKPOINTS: 2 # per iter of open-ended training
PARTNER_POP_SIZE: 1 # true pop size is PARTNER_POP_SIZE * NUM_CHECKPOINTS per iter of open-ended training
SP_WEIGHT: 1.0 # weight on conf-br (SP) loss
REGRET_SP_WEIGHT: 1.0 # weight on the SP term in the perstep regret loss
CONF_OBJ_TYPE: sreg-xp_sreg-sp_ret-sxp # choices: per_state_regret, per_state_regret_target, traj_level_regret, sreg-xp_sreg-sp_ret-sxp, sreg-xp_ret-sp_ret-sxp
REINIT_CONF: true # whether to reinitialize the confederate policy each iteration of open-ended training
REINIT_BR_TO_BR: true # whether to reinitialize the br policy each iteration of open-ended training
REINIT_BR_TO_EGO: false # whether to reinitialize the br policy to the ego params each iteration of open-ended training
EGO_TEAMMATE: final # choices: [final, all] # whether to use the final conf params as the teammate for the ego agent, or all ckpts

# buffer args
SAMPLING_STRATEGY: uniform # 'plr' or 'uniform'
STALENESS_COEF: 0.3 # used only for 'plr' sampling strategy
SCORE_TEMP: 1.0 # used only for 'plr' sampling strategy, currently ineffective
NUM_ENVS: 16
LR: 1.e-4
UPDATE_EPOCHS: 20
NUM_MINIBATCHES: 4
GAMMA: 0.99
GAE_LAMBDA: 0.95
CLIP_EPS: 0.05
ENT_COEF: 0.01
VF_COEF: 0.5
MAX_GRAD_NORM: 1.0
ANNEAL_LR: false